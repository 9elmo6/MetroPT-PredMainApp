{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d38707ff",
   "metadata": {},
   "source": [
    "# SAE Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb23e810",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Conv1D, MaxPooling1D, UpSampling1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import datetime\n",
    "\n",
    "# Load the data\n",
    "raw_data = pd.read_csv('dataset_train.csv', parse_dates=['timestamp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8334e42",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fb0d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df=raw_data.copy()\n",
    "# Compute the difference between consecutive timestamps\n",
    "df['TimeDelta'] = df['timestamp'].diff()\n",
    "\n",
    "# Find the index of the first timestamp for each train\n",
    "train_start_index = df[df['TimeDelta'] > pd.Timedelta(hours=1)].index\n",
    "\n",
    "# Compute the compressor run time and idle time for each train\n",
    "T_run_list = []\n",
    "T_idle_list = []\n",
    "for i in range(len(train_start_index)):\n",
    "    if i < len(train_start_index) - 1:\n",
    "        # For trains that are not the last one\n",
    "        T_run = (df.iloc[train_start_index[i]+1:train_start_index[i+1]]['COMP'] == 1).sum()\n",
    "        T_idle = (df.iloc[train_start_index[i]+1:train_start_index[i+1]]['COMP'] == 0).sum()\n",
    "        T_run_list.append(T_run)\n",
    "        T_idle_list.append(T_idle)\n",
    "    else:\n",
    "        # For the last train\n",
    "        T_run = (df.iloc[train_start_index[i]+1:]['COMP'] == 1).sum()\n",
    "        T_idle = (df.iloc[train_start_index[i]+1:]['COMP'] == 0).sum()\n",
    "        T_run_list.append(T_run)\n",
    "        T_idle_list.append(T_idle)\n",
    "\n",
    "# Add the T_run and T_idle values to the DataFrame\n",
    "df.loc[train_start_index, 'T_run'] = T_run_list\n",
    "df.loc[train_start_index, 'T_idle'] = T_idle_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e6a68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anomalies time intervals\n",
    "anomalies_intervals = [\n",
    "    (pd.Timestamp('2022-02-28 21:50'), pd.Timestamp('2022-03-01 02:00')),\n",
    "    (pd.Timestamp('2022-03-23 12:54'), pd.Timestamp('2022-03-23 15:24')),\n",
    "    (pd.Timestamp('2022-05-30 10:00'), pd.Timestamp('2022-06-02 06:18'))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe84ed08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label the failure regions\n",
    "df['label'] = np.where(\n",
    "    ((df['timestamp'] >= '2022-02-28 21:50') & (df['timestamp'] <= '2022-03-01 02:00')) |\n",
    "    ((df['timestamp'] >= '2022-03-23 14:54') & (df['timestamp'] <= '2022-03-23 15:24')) | \n",
    "    ((df['timestamp'] >= '2022-05-30 12:00') & (df['timestamp'] <= '2022-06-02 06:18')) ,\n",
    "    1, 0)\n",
    "# drop unnecssary columns\n",
    "df=df.drop(['gpsLong','gpsLat','gpsSpeed','gpsQuality'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7002929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all anomalies\n",
    "normal_data = df[df['label'] == 0]\n",
    "\n",
    "anomaly_data_df = df[df['label'] == 1]\n",
    "\n",
    "# Extract some normal data\n",
    "extra_normal_data_df = df[(df['label'] == 0) & \n",
    "                    ((df['timestamp'] >= '2022-02-25') & (df['timestamp'] < '2022-02-28 21:50') |\n",
    "                     (df['timestamp'] >= '2022-03-20 02:00') & (df['timestamp'] < '2022-03-23 14:54'))]       \n",
    "\n",
    "X_true = pd.concat([anomaly_data_df, extra_normal_data_df], axis=0)           \n",
    "y_true = X_true['label'].values\n",
    "X_predict = X_true.drop('label', axis=1)\n",
    "print(X_predict.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a193d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "start_time = pd.Timestamp('2022-05-31 1:00')\n",
    "end_time = pd.Timestamp('2022-05-31 2:18')\n",
    "\n",
    "\n",
    "test_data = df[(df['timestamp'] >= start_time) & (df['timestamp'] <= end_time)]\n",
    "cols_to_plot = X_predict.columns[1:8]  # select columns 1 to 8 (skipping the timestamp column)\n",
    "    \n",
    "fig, axes = plt.subplots(nrows=len(X_predict.columns), ncols=1, figsize=(10,20))\n",
    "for i, col in enumerate(X_predict.columns):\n",
    "    sns.histplot(X_predict[col], ax=axes[i])\n",
    "\n",
    "    axes[i][0].set_title(f'Train {col}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096b8d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df= df.drop('label', axis=1)\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096070e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def segment_intervals(times, n_segments):\n",
    "    return [np.linspace(0, interval, n_segments + 1, endpoint=True, dtype=int) for interval in times]\n",
    "\n",
    "def compute_mean_and_multiply(data, intervals, cycle_duration):\n",
    "    mean_values = []\n",
    "\n",
    "    for interval in intervals:\n",
    "        mean_interval = np.mean(data[interval[0]:interval[-1]])\n",
    "        mean_values.append(mean_interval * cycle_duration)\n",
    "\n",
    "    return mean_values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8189eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_analog(df):\n",
    "    # Preallocate the feature array\n",
    "    num_bins = 7\n",
    "    num_analog_sensors = 8\n",
    "    num_features_per_sensor = num_bins + 2  # num_bins plus T_run and T_idle\n",
    "    features = np.zeros((len(df), num_analog_sensors * num_features_per_sensor))\n",
    "\n",
    "    # Calculate bins for each analog sensor\n",
    "    for sensor_idx in range(num_analog_sensors):\n",
    "        sensor_data = df.iloc[:, sensor_idx + 1]  # Skip the timestamp column\n",
    "\n",
    "        for idx, (T_run, T_idle) in enumerate(zip(df['T_run'], df['T_idle'])):\n",
    "            if pd.isna(T_run) or pd.isna(T_idle):\n",
    "                continue\n",
    "            cycle_duration = T_run + T_idle\n",
    "            T_run_bins = np.array_split(sensor_data[:int(T_run)], 2)\n",
    "            T_idle_bins = np.array_split(sensor_data[int(T_run):int(cycle_duration)], 5)\n",
    "\n",
    "            # Calculate the mean values of each bin\n",
    "            feature_idx = sensor_idx * num_features_per_sensor\n",
    "            features[idx, feature_idx:feature_idx + 2] = [np.mean(bin) for bin in T_run_bins]\n",
    "            features[idx, feature_idx + 2:feature_idx + 7] = [np.mean(bin) for bin in T_idle_bins]\n",
    "            features[idx, feature_idx:feature_idx + 7] *= cycle_duration\n",
    "            features[idx, feature_idx + 7] = T_run\n",
    "            features[idx, feature_idx + 8] = T_idle\n",
    "\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cd5681",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7a94d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Set the initial training and testing time windows\n",
    "start_date_train = pd.Timestamp('2022-01-01')\n",
    "end_date_train = pd.Timestamp('2022-02-01')\n",
    "start_date_test = end_date_train \n",
    "end_date_test = start_date_test + datetime.timedelta(weeks=1)\n",
    "\n",
    "\n",
    "# Set other parameters (num_features, num_epochs, batch_size, etc.) as before\n",
    "num_features = 72\n",
    "num_epochs = 5\n",
    "batch_size = 30\n",
    "beta = 5\n",
    "lamda = 1e-5\n",
    "rho = 0.01\n",
    "alpha = 0.04\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57393acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#parameters\n",
    "num_features = 72\n",
    "num_epochs = 50\n",
    "batch_size = 30\n",
    "beta = 5\n",
    "lamda = 1e-5\n",
    "rho = 0.01\n",
    "alpha = 0.04\n",
    "num_analog_sensors=8\n",
    "num_features_per_sensor=9\n",
    "num_input_features = num_analog_sensors * num_features_per_sensor\n",
    "\n",
    "input_layer = Input(shape=(num_input_features,1), name='input_layer')\n",
    "\n",
    "# Encoder \n",
    "encoder = Conv1D(16, kernel_size=3, activation='relu', padding='same')(input_layer)\n",
    "encoder = MaxPooling1D(pool_size=2, padding='same')(encoder)\n",
    "encoder = Conv1D(8, kernel_size=3, activation='relu', padding='same')(encoder)\n",
    "encoder = MaxPooling1D(pool_size=2, padding='same')(encoder)\n",
    "encoder = Conv1D(8, kernel_size=3, activation='relu', padding='same')(encoder)\n",
    "bottleneck = MaxPooling1D(pool_size=2, padding='same', name='bottleneck')(encoder)\n",
    "\n",
    "#Decoder\n",
    "decoder = Conv1D(8, kernel_size=3, activation='relu', padding='same')(bottleneck)\n",
    "decoder = UpSampling1D(size=2)(decoder)\n",
    "decoder = Conv1D(8, kernel_size=3, activation='relu', padding='same')(decoder)\n",
    "decoder = UpSampling1D(size=2)(decoder)\n",
    "decoder = Conv1D(16, kernel_size=3, activation='relu', padding='same')(decoder)\n",
    "decoder = UpSampling1D(size=2)(decoder)\n",
    "output_layer = Conv1D(1, kernel_size=3, activation='sigmoid', padding='same', name='output_layer')(decoder)\n",
    "\n",
    "sae = Model(input_layer, output_layer)\n",
    "sae.compile(optimizer=Adam(), loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d201e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Encoder architecutre \n",
    "encoder_model = Model(inputs=input_layer, outputs=bottleneck)\n",
    "\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab759c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual training\n",
    "detected_anomalies_indexes=[]\n",
    "while end_date_test <= pd.Timestamp('2022-06-02'):\n",
    "    # Filter out anomaly intervals from the training dataset\n",
    "    mask = df['timestamp'].between(start_date_train, end_date_train) & \\\n",
    "           ~np.any([(df['timestamp'].between(start, end)) for start, end in anomalies_intervals], axis=0)\n",
    "    \n",
    "    train_df = df[mask]\n",
    "    test_df = df[df['timestamp'].between(start_date_test, end_date_test)]\n",
    "\n",
    "    # Extract features for the analog signals\n",
    "    X_train = extract_features_analog(train_df)\n",
    "    X_test = extract_features_analog(test_df)\n",
    "    X_train_reshaped = X_train.reshape(-1, num_features)\n",
    "    X_test_reshaped = X_test.reshape(-1, num_features)\n",
    "\n",
    "\n",
    "    # Normalize the input data\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_reshaped)\n",
    "    X_test_scaled = scaler.transform(X_test_reshaped)\n",
    "\n",
    "    # Train the model\n",
    "    sae.fit(X_train_scaled, X_train_scaled, epochs=num_epochs, batch_size=batch_size, shuffle=True)\n",
    "    X_train_scaled = X_train_scaled.reshape(X_train_scaled.shape[0] ,num_features, 1)\n",
    "\n",
    "\n",
    "    #Predict outputs for training data\n",
    "    X_train_pred = sae.predict(X_train_scaled)\n",
    "    er_train = np.abs(X_train_scaled - X_train_pred)\n",
    "\n",
    "    # Determine threshold using the boxplot on er_train\n",
    "    Q1 = np.percentile(er_train, 25)\n",
    "    Q3 = np.percentile(er_train, 75)\n",
    "    IQR = Q3 - Q1\n",
    "    threshold = Q3 + 1.5 * IQR\n",
    "\n",
    "    # Predict outputs for test data\n",
    "    X_test_scaled = X_test_scaled.reshape(X_test_scaled.shape[0] ,num_features, 1)\n",
    "    X_test_pred = sae.predict(X_test_scaled)\n",
    "\n",
    "    er_test = np.abs(X_test_scaled - X_test_pred)\n",
    "\n",
    "    # Apply Low Pass Filter (LPF)\n",
    "    er_filtered = alpha * er_test[:-1] + (1 - alpha) * er_test[1:]\n",
    "\n",
    "    # Compute er_thresholding by comparing the filtered er_test with the threshold\n",
    "    er_thresholding = er_filtered > threshold\n",
    "    # Detect and remove anomalies from the original DataFrame\n",
    "    anomalies = np.where(er_thresholding)\n",
    "    print(\"Detected anomalies on test data:\", anomalies)\n",
    "    anomalies_indexes = np.where(er_thresholding)[0] + test_df.index[0]\n",
    "    detected_anomalies_indexes.extend(anomalies_indexes.tolist())\n",
    "    df = df.drop(anomalies_indexes, axis=0)\n",
    "\n",
    "\n",
    "    # Move the time window one week forward\n",
    "    start_date_train += datetime.timedelta(weeks=1)\n",
    "    end_date_train += datetime.timedelta(weeks=1)\n",
    "    start_date_test += datetime.timedelta(weeks=1)\n",
    "    end_date_test += datetime.timedelta(weeks=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5019b866",
   "metadata": {},
   "outputs": [],
   "source": [
    "sae.save('firstsae.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b6c6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('firstsae.pkl', 'wb') as f:\n",
    "    pickle.dump(sae, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4790f71f",
   "metadata": {},
   "source": [
    "## Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d70e6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "sae = tf.keras.models.load_model('firstsae.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3661a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,classification_report, confusion_matrix\n",
    "\n",
    "X_pred_raw = extract_features_analog(X_predict)\n",
    "X_pred = scaler.transform(X_pred_raw)\n",
    "X_pred = X_pred.reshape(X_pred.shape[0] ,num_features, 1)\n",
    "X_pred_outputs = sae.predict(X_pred)\n",
    "X_pred_outputs.shape\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bf6be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compute reconstruction error for X_pred\n",
    "er_pred = np.abs(X_pred - X_pred_outputs)\n",
    "\n",
    "# Compare with threshold\n",
    "er_pred_thresholding = er_pred_filtered > threshold\n",
    "\n",
    "# Assign anomaly labels (1 for anomaly, 0 for normal) based on above comaprison\n",
    "y_pred = er_pred_thresholding.astype(int)\n",
    "y_true = y_true[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56de7e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating ground truth array based on known failure regions\n",
    "y_true_binary = np.zeros_like(X_pred)\n",
    "\n",
    "# Set the anomaly rows to 1\n",
    "y_true_binary[:anomaly_data_df.shape[0],:,:] = 1\n",
    "print(y_true_binary.shape)\n",
    "y_true_binary = y_true_binary[:-1]\n",
    "\n",
    "y_true_binary.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849bb921",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reshape y_pred and y_true_binary to 1D arrays\n",
    "y_pred_flat = np.ravel(y_pred)\n",
    "y_true_binary_flat = np.ravel(y_true_binary)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_true_binary_flat, y_pred_flat)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Compute classification report (precision, recall, and F1 score)\n",
    "report = classification_report(y_true_binary_flat, y_pred_flat)\n",
    "print(\"Classification report:\")\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
