{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb23e810",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data into a Pandas DataFrame\n",
    "raw_data = pd.read_csv('dataset_train.csv', parse_dates=['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52fb0d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df=raw_data.copy()\n",
    "# Compute the difference between consecutive timestamps\n",
    "df['TimeDelta'] = df['timestamp'].diff()\n",
    "\n",
    "# Find the index of the first timestamp for each train\n",
    "train_start_index = df[df['TimeDelta'] > pd.Timedelta(hours=1)].index\n",
    "\n",
    "# Compute the compressor run time and idle time for each train\n",
    "T_run_list = []\n",
    "T_idle_list = []\n",
    "for i in range(len(train_start_index)):\n",
    "    if i < len(train_start_index) - 1:\n",
    "        # For trains that are not the last one\n",
    "        T_run = (df.iloc[train_start_index[i]+1:train_start_index[i+1]]['COMP'] == 1).sum()\n",
    "        T_idle = (df.iloc[train_start_index[i]+1:train_start_index[i+1]]['COMP'] == 0).sum()\n",
    "        T_run_list.append(T_run)\n",
    "        T_idle_list.append(T_idle)\n",
    "    else:\n",
    "        # For the last train\n",
    "        T_run = (df.iloc[train_start_index[i]+1:]['COMP'] == 1).sum()\n",
    "        T_idle = (df.iloc[train_start_index[i]+1:]['COMP'] == 0).sum()\n",
    "        T_run_list.append(T_run)\n",
    "        T_idle_list.append(T_idle)\n",
    "\n",
    "# Add the T_run and T_idle values to the DataFrame\n",
    "df.loc[train_start_index, 'T_run'] = T_run_list\n",
    "df.loc[train_start_index, 'T_idle'] = T_idle_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67e6a68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anomalies time intervals\n",
    "\n",
    "anomalies_intervals = [\n",
    "    (pd.Timestamp('2022-02-28 21:50'), pd.Timestamp('2022-03-01 02:00')),\n",
    "    (pd.Timestamp('2022-03-23 12:54'), pd.Timestamp('2022-03-23 15:24')),\n",
    "    (pd.Timestamp('2022-05-30 10:00'), pd.Timestamp('2022-06-02 06:18'))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe84ed08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df['label'] = np.where(\n",
    "    ((df['timestamp'] >= '2022-02-28 21:50') & (df['timestamp'] <= '2022-03-01 02:00')) |\n",
    "    ((df['timestamp'] >= '2022-03-23 14:54') & (df['timestamp'] <= '2022-03-23 15:24')) | \n",
    "    ((df['timestamp'] >= '2022-05-30 12:00') & (df['timestamp'] <= '2022-06-02 06:18')) ,\n",
    "    1, 0)\n",
    "\n",
    "df=df.drop(['gpsLong','gpsLat','gpsSpeed','gpsQuality'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7002929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(740534, 20)\n"
     ]
    }
   ],
   "source": [
    "# Extract all anomalies\n",
    "normal_data = df[df['label'] == 0]\n",
    "\n",
    "anomaly_data_df = df[df['label'] == 1]\n",
    "\n",
    "# Extract some normal data\n",
    "extra_normal_data_df = df[(df['label'] == 0) & \n",
    "                    ((df['timestamp'] >= '2022-02-25') & (df['timestamp'] < '2022-02-28 21:50') |\n",
    "                     (df['timestamp'] >= '2022-03-20 02:00') & (df['timestamp'] < '2022-03-23 14:54'))]       \n",
    "\n",
    "X_true = pd.concat([anomaly_data_df, extra_normal_data_df], axis=0)           \n",
    "y_true = X_true['label'].values\n",
    "X_predict = X_true.drop('label', axis=1)\n",
    "print(X_predict.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "096b8d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10773588, 20)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "df= df.drop('label', axis=1)\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "096070e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def segment_intervals(times, n_segments):\n",
    "    return [np.linspace(0, interval, n_segments + 1, endpoint=True, dtype=int) for interval in times]\n",
    "\n",
    "def compute_mean_and_multiply(data, intervals, cycle_duration):\n",
    "    mean_values = []\n",
    "\n",
    "    for interval in intervals:\n",
    "        mean_interval = np.mean(data[interval[0]:interval[-1]])\n",
    "        mean_values.append(mean_interval * cycle_duration)\n",
    "\n",
    "    return mean_values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b8189eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_analog(df):\n",
    "    # Preallocate the feature array\n",
    "    num_bins = 7\n",
    "    num_analog_sensors = 8\n",
    "    num_features_per_sensor = num_bins + 2  # num_bins plus T_run and T_idle\n",
    "    features = np.zeros((len(df), num_analog_sensors * num_features_per_sensor))\n",
    "\n",
    "    # Calculate bins for each analog sensor\n",
    "    for sensor_idx in range(num_analog_sensors):\n",
    "        sensor_data = df.iloc[:, sensor_idx + 1]  # Skip the timestamp column\n",
    "\n",
    "        for idx, (T_run, T_idle) in enumerate(zip(df['T_run'], df['T_idle'])):\n",
    "            # Check for NaN values in T_run and T_idle\n",
    "            if pd.isna(T_run) or pd.isna(T_idle):\n",
    "                continue\n",
    "\n",
    "            cycle_duration = T_run + T_idle\n",
    "            T_run_bins = np.array_split(sensor_data[:int(T_run)], 2)\n",
    "            T_idle_bins = np.array_split(sensor_data[int(T_run):int(cycle_duration)], 5)\n",
    "\n",
    "            # Calculate the mean values of each bin\n",
    "            feature_idx = sensor_idx * num_features_per_sensor\n",
    "            features[idx, feature_idx:feature_idx + 2] = [np.mean(bin) for bin in T_run_bins]\n",
    "            features[idx, feature_idx + 2:feature_idx + 7] = [np.mean(bin) for bin in T_idle_bins]\n",
    "\n",
    "            # Multiply the mean value by the cycle duration\n",
    "            features[idx, feature_idx:feature_idx + 7] *= cycle_duration\n",
    "\n",
    "            # Add the T_run and T_idle values to the features\n",
    "            features[idx, feature_idx + 7] = T_run\n",
    "            features[idx, feature_idx + 8] = T_idle\n",
    "\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec7a94d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import datetime\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Set the initial training and testing time windows\n",
    "start_date_train = pd.Timestamp('2022-01-01')\n",
    "end_date_train = pd.Timestamp('2022-02-01')\n",
    "start_date_test = end_date_train \n",
    "end_date_test = start_date_test + datetime.timedelta(weeks=1)\n",
    "\n",
    "\n",
    "# Set other parameters (num_features, num_epochs, batch_size, etc.) as before\n",
    "num_features = 72\n",
    "num_epochs = 5\n",
    "batch_size = 30\n",
    "beta = 5\n",
    "lamda = 1e-5\n",
    "rho = 0.01\n",
    "alpha = 0.04\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebce2812",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Conv1D, MaxPooling1D, UpSampling1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab759c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set other parameters\n",
    "num_features = 72\n",
    "num_epochs = 50\n",
    "batch_size = 30\n",
    "beta = 5\n",
    "lamda = 1e-5\n",
    "rho = 0.01\n",
    "alpha = 0.04\n",
    "num_analog_sensors=8\n",
    "num_features_per_sensor=9\n",
    "num_input_features = num_analog_sensors * num_features_per_sensor\n",
    "\n",
    "input_layer = Input(shape=(num_input_features,1), name='input_layer')\n",
    "\n",
    "# Encoder layers\n",
    "encoder = Conv1D(16, kernel_size=3, activation='relu', padding='same')(input_layer)\n",
    "encoder = MaxPooling1D(pool_size=2, padding='same')(encoder)\n",
    "encoder = Conv1D(8, kernel_size=3, activation='relu', padding='same')(encoder)\n",
    "encoder = MaxPooling1D(pool_size=2, padding='same')(encoder)\n",
    "encoder = Conv1D(8, kernel_size=3, activation='relu', padding='same')(encoder)\n",
    "bottleneck = MaxPooling1D(pool_size=2, padding='same', name='bottleneck')(encoder)\n",
    "\n",
    "# Decoder layers\n",
    "decoder = Conv1D(8, kernel_size=3, activation='relu', padding='same')(bottleneck)\n",
    "decoder = UpSampling1D(size=2)(decoder)\n",
    "decoder = Conv1D(8, kernel_size=3, activation='relu', padding='same')(decoder)\n",
    "decoder = UpSampling1D(size=2)(decoder)\n",
    "decoder = Conv1D(16, kernel_size=3, activation='relu', padding='same')(decoder)\n",
    "decoder = UpSampling1D(size=2)(decoder)\n",
    "output_layer = Conv1D(1, kernel_size=3, activation='sigmoid', padding='same', name='output_layer')(decoder)\n",
    "\n",
    "sae = Model(input_layer, output_layer)\n",
    "sae.compile(optimizer=Adam(), loss='mean_squared_error')\n",
    "detected_anomalies_indexes=[]\n",
    "while end_date_test <= pd.Timestamp('2022-06-02'):\n",
    "    # Filter out anomaly intervals from the training dataset\n",
    "    mask = df['timestamp'].between(start_date_train, end_date_train) & \\\n",
    "           ~np.any([(df['timestamp'].between(start, end)) for start, end in anomalies_intervals], axis=0)\n",
    "    \n",
    "    train_df = df[mask]\n",
    "    test_df = df[df['timestamp'].between(start_date_test, end_date_test)]\n",
    "\n",
    "    # Extract features for the analog signals\n",
    "    X_train = extract_features_analog(train_df)\n",
    "    X_test = extract_features_analog(test_df)\n",
    "    X_train_reshaped = X_train.reshape(-1, num_features)\n",
    "    X_test_reshaped = X_test.reshape(-1, num_features)\n",
    "\n",
    "\n",
    "    # Normalize the input data\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_reshaped)\n",
    "    X_test_scaled = scaler.transform(X_test_reshaped)\n",
    "\n",
    "    # Train the SAE model\n",
    "    sae.fit(X_train_scaled, X_train_scaled, epochs=num_epochs, batch_size=batch_size, shuffle=True)\n",
    "    X_train_scaled = X_train_scaled.reshape(X_train_scaled.shape[0] ,num_features, 1)\n",
    "\n",
    "\n",
    "    #Predict outputs for training data with the SAE\n",
    "    X_train_pred = sae.predict(X_train_scaled)\n",
    "\n",
    "    # Compute reconstruction error for training data\n",
    "    er_train = np.abs(X_train_scaled - X_train_pred)\n",
    "\n",
    "    # Determine threshold using the boxplot on er_train\n",
    "    Q1 = np.percentile(er_train, 25)\n",
    "    Q3 = np.percentile(er_train, 75)\n",
    "    IQR = Q3 - Q1\n",
    "    threshold = Q3 + 1.5 * IQR\n",
    "\n",
    "    # Predict outputs for test data with the SAE\n",
    "    X_test_scaled = X_test_scaled.reshape(X_test_scaled.shape[0] ,num_features, 1)\n",
    "    X_test_pred = sae.predict(X_test_scaled)\n",
    "\n",
    "\n",
    "    # Compute reconstruction error for test data\n",
    "    er_test = np.abs(X_test_scaled - X_test_pred)\n",
    "\n",
    "    # Apply the Low Pass Filter (LPF) to er_test\n",
    "    er_filtered = alpha * er_test[:-1] + (1 - alpha) * er_test[1:]\n",
    "\n",
    "    # Compute er_thresholding by comparing the filtered er_test with the threshold\n",
    "    er_thresholding = er_filtered > threshold\n",
    "     # Output: Detected anomalies on test data\n",
    "    anomalies = np.where(er_thresholding)\n",
    "    print(\"Detected anomalies on test data:\", anomalies)\n",
    "     # Append the detected anomalies' indexes\n",
    "    anomalies_indexes = np.where(er_thresholding)[0] + test_df.index[0]\n",
    "    detected_anomalies_indexes.extend(anomalies_indexes.tolist())\n",
    "\n",
    "    # Remove detected anomalies from the original DataFrame\n",
    "    df = df.drop(anomalies_indexes, axis=0)\n",
    "\n",
    "    # Move the time window one week forward\n",
    "    start_date_train += datetime.timedelta(weeks=1)\n",
    "    end_date_train += datetime.timedelta(weeks=1)\n",
    "    start_date_test += datetime.timedelta(weeks=1)\n",
    "    end_date_test += datetime.timedelta(weeks=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3661a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "X_pred_raw = extract_features_analog(X_predict)  # Replace new_data with the actual new data you want to predict anomalies for\n",
    "X_pred = scaler.transform(X_pred_raw)\n",
    "X_pred = X_pred.reshape(X_pred.shape[0] ,num_features, 1)\n",
    "\n",
    "\n",
    "# Predict outputs for the new data with the SAE\n",
    "X_pred_outputs = sae.predict(X_pred)\n",
    "X_pred_outputs.shape\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bf6be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compute reconstruction error for X_pred\n",
    "er_pred = np.abs(X_pred - X_pred_outputs)\n",
    "\n",
    "# Apply the Low Pass Filter (LPF) to er_pred\n",
    "er_pred_filtered = alpha * er_pred[:-1] + (1 - alpha) * er_pred[1:]\n",
    "\n",
    "# Compute er_thresholding by comparing the filtered er_pred with the threshold\n",
    "er_pred_thresholding = er_pred_filtered > threshold\n",
    "\n",
    "# Assign anomaly labels (1 for anomaly, 0 for normal) based on er_thresholding\n",
    "y_pred = er_pred_thresholding.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7f07d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_true[:-1]\n",
    "y_true_binary = np.zeros_like(X_pred)\n",
    "\n",
    "# Set the anomaly rows to 1\n",
    "y_true_binary[:anomaly_data_df.shape[0],:,:] = 1\n",
    "print(y_true_binary.shape)\n",
    "\n",
    "y_true_binary.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849bb921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "\n",
    "# Reshape y_pred and y_true_binary to 1D arrays\n",
    "y_pred_flat = np.ravel(y_pred)\n",
    "y_true_binary_flat = np.ravel(y_true_binary)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_true_binary_flat, y_pred_flat)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Compute classification report (precision, recall, and F1 score)\n",
    "report = classification_report(y_true_binary_flat, y_pred_flat)\n",
    "print(\"Classification report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d37bf71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
